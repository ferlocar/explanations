{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4282857, 15)\n",
      "(1573876, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>target_age</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>631f821671a94f991413e58c2b393b29</td>\n",
       "      <td>False</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bdeb2156021c5da9237f5d2667ab590a</td>\n",
       "      <td>False</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>d9fb05627d4659484703e5827690b9f9</td>\n",
       "      <td>False</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>adc399722fccda28049cb26c5a795b16</td>\n",
       "      <td>False</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18f92f9c1f86b5cd692e8fcc19721b1d</td>\n",
       "      <td>False</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              userid  target_age   age\n",
       "6   631f821671a94f991413e58c2b393b29       False  30.0\n",
       "7   bdeb2156021c5da9237f5d2667ab590a       False  29.0\n",
       "12  d9fb05627d4659484703e5827690b9f9       False  27.0\n",
       "23  adc399722fccda28049cb26c5a795b16       False  29.0\n",
       "29  18f92f9c1f86b5cd692e8fcc19721b1d       False  27.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Prepare people data frame\n",
    "df_people = pd.read_csv('Data/demog.csv')\n",
    "print(df_people.shape)\n",
    "# Keep only people for which we have age\n",
    "df_people = df_people[~pd.isna(df_people.age)]\n",
    "print(df_people.shape)\n",
    "# Target teenagers\n",
    "df_people['target_age'] = df_people.age <= 19\n",
    "df_people = df_people[['userid', 'target_age', 'age']]\n",
    "df_people.userid = df_people.userid.astype('category')\n",
    "df_people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55775701, 2)\n"
     ]
    }
   ],
   "source": [
    "# Prepare likes data frame\n",
    "df_likes = pd.read_csv('Data/likes.csv', header=None, names=['userid', 'like_id'])\n",
    "df_likes.userid = df_likes.userid.astype('category')\n",
    "print(df_likes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes before and after\n",
      "(55775701, 2)\n",
      "(37686388, 2)\n",
      "People before and after\n",
      "(1573876, 3)\n",
      "(588610, 3)\n",
      "Pages before and after\n",
      "15914\n",
      "10822\n",
      "Likes before and after\n",
      "(33873012, 2)\n",
      "(33873012, 2)\n",
      "People before and after\n",
      "(588610, 3)\n",
      "(587745, 3)\n",
      "Pages before and after\n",
      "10822\n",
      "10822\n",
      "Likes before and after\n",
      "(33873012, 2)\n",
      "(33873012, 2)\n",
      "People before and after\n",
      "(587745, 3)\n",
      "(587745, 3)\n",
      "Pages before and after\n",
      "10822\n",
      "10822\n"
     ]
    }
   ],
   "source": [
    "# Prune to keep only pages with more than 1,000 users and users with at least one like\n",
    "change = True\n",
    "while change:\n",
    "    # Keep only likes of people for which we have information\n",
    "    print(\"Likes before and after\")\n",
    "    val = df_likes.shape[0]\n",
    "    print(df_likes.shape)\n",
    "    df_likes = df_likes[df_likes.userid.isin(df_people.userid)]\n",
    "    print(df_likes.shape)\n",
    "    change = val != df_likes.shape[0]\n",
    "\n",
    "    # Keep only people with at least one like\n",
    "    print(\"People before and after\")\n",
    "    val = df_people.shape[0]\n",
    "    print(df_people.shape)\n",
    "    df_people = df_people[df_people.userid.isin(df_likes.userid.unique())]\n",
    "    print(df_people.shape)\n",
    "    change = (val != df_people.shape[0]) | change\n",
    "    # Keep only pages with at least 1,000 likes\n",
    "    print(\"Pages before and after\")\n",
    "    counts = df_likes.like_id.value_counts()\n",
    "    val = len(counts)\n",
    "    high_counts = counts[counts >= 1000]\n",
    "    change = (val != len(high_counts)) | change\n",
    "    print(len(counts))\n",
    "    df_likes = df_likes[df_likes.like_id.isin(high_counts.index.values)]\n",
    "    print(len(high_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<587745x10822 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 33873012 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get labels\n",
    "labels = df_people.set_index('userid').loc[sorted(df_likes.userid.unique())].target_age.values\n",
    "ages = df_people.set_index('userid').loc[sorted(df_likes.userid.unique())].age.values\n",
    "# Build sparse matrix\n",
    "users_c = np.array(sorted(df_likes.userid.unique()))\n",
    "pages_c = np.array(sorted(df_likes.like_id.unique()))\n",
    "row = pd.Categorical(df_likes.userid, categories=users_c, ordered=True).codes\n",
    "col = pd.Categorical(df_likes.like_id, categories=pages_c, ordered=True).codes\n",
    "sparse_mat = csr_matrix((np.ones(df_likes.shape[0]), (row, col)), shape=(users_c.size, pages_c.size))\n",
    "sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferlo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8727784573849582"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build machine learning model\n",
    "X_train, X_test, y_train, y_test, age_train, age_test = train_test_split(sparse_mat, labels, ages,\n",
    "                                                                         test_size=0.3, random_state=42)\n",
    "\n",
    "model = LogisticRegression(random_state=42, solver='lbfgs').fit(X_train, y_train)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19041200, 2)\n",
      "(10821, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9254817079</th>\n",
       "      <td>photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103144503058854</th>\n",
       "      <td>Poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104046862964234</th>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105667052799444</th>\n",
       "      <td>Piano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106059522759137</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name\n",
       "like_id                     \n",
       "9254817079       photography\n",
       "103144503058854       Poetry\n",
       "104046862964234      Writing\n",
       "105667052799444        Piano\n",
       "106059522759137      English"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load names of facebook pages\n",
    "df_names = pd.read_csv('Data/fb_like.csv', encoding='latin-1', error_bad_lines=False, warn_bad_lines=False)\n",
    "print(df_names.shape)\n",
    "df_names.head()\n",
    "df_names = df_names[df_names.like_id.isin(pages_c)]\n",
    "print(df_names.shape)\n",
    "df_names = df_names.set_index('like_id')\n",
    "df_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794988674752503\n",
      "276.2428263581717\n",
      "157.0\n",
      "Errors of people between 30 and 50 years\n",
      "99\n",
      "[0.98169237 0.99896217 0.97713391 0.99986298 0.9476107  0.85177353\n",
      " 0.9951256  0.9983207  0.95869546 0.98730223 0.94229269 0.99100626\n",
      " 0.99347797 0.99317242 0.8136403  0.96125115 0.92354525 0.96041856\n",
      " 0.99998922 0.81137904 0.85491682 0.99958716 0.99105258 0.92894244\n",
      " 0.81376887 0.99999999 0.80162073 0.94488282 0.95440956 0.9763705\n",
      " 0.86121731 0.94134376 0.96671992 0.89188035 0.8974984  0.99354177\n",
      " 0.8932332  0.99511592 0.85857827 0.8316827  0.99988883 0.80858969\n",
      " 0.8293599  0.99472555 0.94824506 0.97017833 0.99999921 0.99688985\n",
      " 0.83159057 0.97975629 0.88912899 0.89665586 0.96513789 0.99885476\n",
      " 0.99813589 0.8396682  0.98670352 0.99598254 0.999205   0.87260677\n",
      " 0.99463161 0.98667583 0.85896881 0.84546594 0.94797361 0.99897486\n",
      " 0.80712154 0.96552465 0.87577587 0.99979524 0.8072721  0.9105793\n",
      " 0.85879328 0.98755166 0.99885857 0.99957779 0.90147285 0.99894757\n",
      " 0.95510547 0.85074211 0.85197809 0.99622463 0.88020927 0.98600041\n",
      " 0.97775362 0.98972551 0.9560058  0.86336169 0.80774672 0.92411915\n",
      " 0.83775851 0.90853262 0.99132047 0.80298877 0.95274211 0.85253902\n",
      " 0.95520248 0.86287952 0.91909821]\n",
      "[[ 324.  240. 1247.  211.   87.   16.  202.  209.  193.  434.   87.  254.\n",
      "   110.  224.   27.  693.  126.   92.  351.  108.   96.  348.  810.  398.\n",
      "   411.  897.  127.  159.  111.  559.  304.   96.  221.   59.  397.   82.\n",
      "   206.  512.  214.   67. 1300.   42.  158.  593.   72.  219.  438.   97.\n",
      "    20.  292.   42.   60.   90.  165.  516.   58.  139.  220.  489.  323.\n",
      "   430.  158.   77.  157.  238.  219.   25.  163.  740.  419.   69.   28.\n",
      "    95.   89.  324.  324.   44.  215.   68.   62.   37.   62.  157.  314.\n",
      "   662.  248.   43.   74.   17.   52.  121.  165.  188.  266.   82.   37.\n",
      "    61.  104.   53.]]\n"
     ]
    }
   ],
   "source": [
    "# Get threshold\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "threshold = np.percentile(probs, 95)\n",
    "print(threshold)\n",
    "# Keep only people with probabilities above threshold\n",
    "top_x = X_test[probs > threshold, :]\n",
    "top_y = y_test[probs > threshold]\n",
    "top_age = age_test[probs > threshold]\n",
    "# Take a look at average number of likes\n",
    "print(top_x.sum(axis=1).mean())\n",
    "print(np.median(np.array(top_x.sum(axis=1))))\n",
    "# Keep interesting errors\n",
    "top_x = top_x[top_age >= 30]\n",
    "top_y = top_y[top_age >= 30]\n",
    "top_age = top_age[top_age >= 30]\n",
    "top_x = top_x[top_age <= 50]\n",
    "top_y = top_y[top_age <= 50]\n",
    "top_age = top_age[top_age <= 50]\n",
    "print(\"Errors of people between 30 and 50 years\")\n",
    "print(top_x.shape[0])\n",
    "# Take a look at probabilities\n",
    "print(model.predict_proba(top_x)[:, 1])\n",
    "print(top_x.sum(axis=1).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 37% of targets have made at least 227 likes\n",
    "targets = model.predict_proba(X_test)[:, 1] > threshold\n",
    "np.percentile(X_test[targets, :].sum(axis=1), 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.829359899218118"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(top_x)[42, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Position:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like_id\n",
      "23625182151        Call of Duty: World at War\n",
      "76377189413                        Heidi Klum\n",
      "54244944269                        Demi Moore\n",
      "42798291365             SpongeBob SquarePants\n",
      "282322361806841                      The Sims\n",
      "Name: name, dtype: object\n",
      "[0.14280135 0.11432135 0.10700707 0.0978031  0.07529212]\n",
      "like_id\n",
      "23625182151        Call of Duty: World at War\n",
      "78041983445                        Chocolate!\n",
      "7427438063                         The Sims 2\n",
      "282322361806841                      The Sims\n",
      "54244944269                        Demi Moore\n",
      "Name: name, dtype: object\n",
      "[0.10451007 0.10240373 0.09898086 0.09107538 0.08861605]\n",
      "like_id\n",
      "54244944269                        Demi Moore\n",
      "23625182151        Call of Duty: World at War\n",
      "76377189413                        Heidi Klum\n",
      "282322361806841                      The Sims\n",
      "56470448215                              Puma\n",
      "Name: name, dtype: object\n",
      "[0.12183726 0.11313927 0.10255483 0.10190203 0.07478998]\n",
      "like_id\n",
      "76377189413            Heidi Klum\n",
      "54244944269            Demi Moore\n",
      "200673103280250     Magic Johnson\n",
      "39399781765        Boston Red Sox\n",
      "321662419491        Google Chrome\n",
      "Name: name, dtype: object\n",
      "[0.1175683  0.11144469 0.10192208 0.09102381 0.08602579]\n",
      "like_id\n",
      "78041983445                        Chocolate!\n",
      "76377189413                        Heidi Klum\n",
      "23625182151        Call of Duty: World at War\n",
      "282322361806841                      The Sims\n",
      "39399781765                    Boston Red Sox\n",
      "Name: name, dtype: object\n",
      "[0.1114156  0.10040758 0.0906266  0.07990921 0.07528812]\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Candidates 12, 3, 15, 21\n",
    "# Candidates 32, 74, 88, 39, 41, 46\n",
    "\n",
    "def make_decision(data):\n",
    "    return (model.predict_proba(data)[:, 1] > threshold).astype(float)\n",
    "\n",
    "#for i in range(99):\n",
    "i = 32\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Position: \", i)\n",
    "top_x_guy = top_x[np.full(5, i), :]\n",
    "np.random.seed(0)\n",
    "default = csr_matrix(np.zeros(X_test.shape[1]))\n",
    "shap_explainer = shap.KernelExplainer(make_decision, default)\n",
    "shap_values = shap_explainer.shap_values(top_x_guy, nsamples=4000, l1_reg='aic')\n",
    "\n",
    "# Top 5 predictive features are different every time. \n",
    "for i in range(shap_values.shape[0]):\n",
    "    i_vals = (-shap_values[i, :]).argsort()\n",
    "    print(df_names.loc[pages_c[i_vals[:5]]].name)\n",
    "    print(shap_values[i, i_vals[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[221.],\n",
       "        [221.],\n",
       "        [221.],\n",
       "        [221.],\n",
       "        [221.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of websites liked by the guy\n",
    "top_x_guy.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11967897415161133\n",
      "Explanations found\n",
      "45\n",
      "Length of explanation\n",
      "4\n",
      "First explanation\n",
      "                                   name\n",
      "like_id                                \n",
      "23625182151  Call of Duty: World at War\n",
      "54244944269                  Demi Moore\n",
      "76377189413                  Heidi Klum\n",
      "78041983445                  Chocolate!\n",
      "                                       name\n",
      "like_id                                    \n",
      "23625182151      Call of Duty: World at War\n",
      "54244944269                      Demi Moore\n",
      "76377189413                      Heidi Klum\n",
      "282322361806841                    The Sims\n",
      "                                   name\n",
      "like_id                                \n",
      "23625182151  Call of Duty: World at War\n",
      "39399781765              Boston Red Sox\n",
      "54244944269                  Demi Moore\n",
      "76377189413                  Heidi Klum\n",
      "                                       name\n",
      "like_id                                    \n",
      "23625182151      Call of Duty: World at War\n",
      "54244944269                      Demi Moore\n",
      "76377189413                      Heidi Klum\n",
      "200673103280250               Magic Johnson\n",
      "                                    name\n",
      "like_id                                 \n",
      "23625182151   Call of Duty: World at War\n",
      "54244944269                   Demi Moore\n",
      "76377189413                   Heidi Klum\n",
      "321662419491               Google Chrome\n"
     ]
    }
   ],
   "source": [
    "# See how long it takes for our method to find one explanation\n",
    "import explainer\n",
    "import time\n",
    "\n",
    "def scoring_function(data):\n",
    "    return model.predict_proba(data)[:, 1]\n",
    "\n",
    "# Get evidence-based explanations\n",
    "explain = explainer.Explainer(scoring_function, default, prune=False)\n",
    "t0 = time.time()\n",
    "explanations = explain.explain(top_x_guy[0, :], thresholds=float(threshold), max_ite=50)\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "print(\"Explanations found\")\n",
    "print(len(explanations[0]))\n",
    "print(\"Length of explanation\")\n",
    "print(len(explanations[0][0]))\n",
    "print(\"First explanation\")\n",
    "print(df_names.loc[pages_c[explanations[0][0]]])\n",
    "print(df_names.loc[pages_c[explanations[0][1]]])\n",
    "print(df_names.loc[pages_c[explanations[0][2]]])\n",
    "print(df_names.loc[pages_c[explanations[0][3]]])\n",
    "print(df_names.loc[pages_c[explanations[0][4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 1, 1, 3, 4, 0, 3, 2, 1, 3, 2, 1, 1, 0, 1, 5, 0, 3, 3, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 2, 2, 2, 2, 2, 3, 1, 0, 0, 4, 1, 0, 2, 1, 3, 3, 3, 0, 5, 1, 3, 0, 1, 3, 4, 1, 3, 4, 3, 3, 3, 5, 1, 3, 3, 2, 1, 2, 2, 2, 1, 0, 2, 3, 5, 3, 3, 1, 0, 0, 1, 1, 1, 1, 4, 2, 0, 3, 4, 3, 3, 4, 3, 1, 0, 5, 2, 2, 4, 1, 2, 3, 3, 2, 4, 2, 0, 1, 0, 2, 3, 1, 3, 3, 1, 2, 4, 0, 0, 3, 1, 1, 4, 4, 0, 3, 2, 2, 2, 2, 0, 0, 0, 3, 3, 1, 1, 2, 4, 3, 0, 5, 5, 2, 3, 3, 3, 4, 3, 4, 1, 2, 2, 0, 1, 0, 3, 2, 1, 2, 0, 5, 3, 1, 2, 2, 0, 2, 0, 1, 4, 0, 2, 1, 4, 3, 3, 1, 0, 0, 0, 0, 1, 3, 0, 0, 0, 3, 2, 0, 0, 0, 4, 2, 3, 3, 1, 1, 0, 0, 2, 1, 0, 3, 1, 0, 4, 3, 1, 2, 3, 3, 0, 4, 1, 4, 0, 0, 0, 1, 3, 1, 0, 2, 2, 0, 4, 1, 4, 1, 0, 0, 1, 1, 0, 2, 0, 3, 4, 0, 0, 2, 1, 2, 4, 1, 3, 4, 0, 3, 3, 0, 3, 3, 3, 2, 4, 3, 3, 0, 2, 4, 2, 0, 1, 3, 0, 2, 1, 1, 2, 2, 1, 4, 2, 2, 0, 3, 4, 1, 0, 1, 3, 4, 0, 1, 4, 1, 3, 1, 2, 0, 2, 4, 1, 1, 4, 0, 3, 4, 0, 2, 3, 1, 2, 2, 0, 5, 2, 3, 4, 1, 2, 0, 2, 0, 1, 1, 1, 0, 3, 0, 2, 2, 0, 1, 4, 1, 2, 0, 0, 4, 4, 3, 0, 2, 0, 3, 4, 2, 0, 1, 1, 3, 1, 4, 3, 1, 2, 0, 4, 1, 1, 0, 2, 1, 1, 1, 2, 3, 2, 3, 4, 3, 0, 1, 0, 2, 0, 4, 0, 1, 2, 1, 1, 3, 3, 0, 0, 1, 3, 0, 1, 2, 3, 4, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 1, 0, 1, 3, 3, 2, 3, 0, 2, 1, 4, 2, 1, 4, 3, 4, 3, 3, 3, 0, 4, 4, 0, 0, 2, 0, 3, 3, 4, 0, 1, 1, 3, 2, 3, 1, 1, 0, 4, 0, 0, 4, 1, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 3, 3, 1, 0, 3, 4, 3, 2, 2, 4, 2, 0, 2, 0, 3, 3, 1, 1, 0, 4, 1, 3, 3, 3, 2, 2, 3, 0, 2, 1, 1, 2, 4, 1, 3, 1, 2, 4, 2, 4, 1, 0, 0, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "sample_size = 500\n",
    "np.random.seed(0)\n",
    "# Get sample of targeted observations to explain\n",
    "X_targeted = X_test[probs > threshold, :]\n",
    "X_sample = X_targeted[np.random.choice(X_targeted.shape[0], sample_size, replace=False), :]\n",
    "X_to_explain = X_sample[np.arange(X_sample.shape[0]).repeat(5), :]\n",
    "# Obtain their SHAP values\n",
    "#shap_values = shap_explainer.shap_values(X_to_explain, nsamples=4000, l1_reg='aic')\n",
    "#np.save(\"back_up_shap_values.npy\", shap_values)\n",
    "shap_values = np.load(\"Data/back_up_shap_values.npy\")\n",
    "# Find top 5 most informative features\n",
    "most_informative_f = np.argsort(shap_values, axis=1)[:, -5:]\n",
    "# Compute number of matches\n",
    "sample_matches = list()\n",
    "for i in range(sample_size):\n",
    "    matches = reduce(np.intersect1d, most_informative_f[(i*5):((i+1)*5), :]).size\n",
    "    sample_matches.append(matches)\n",
    "    \n",
    "print(sample_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper limit:  60.80000000000001\n",
      "2.87\n",
      "Upper limit:  114.60000000000002\n",
      "2.37\n",
      "Upper limit:  202.0\n",
      "2.0202020202020203\n",
      "Upper limit:  442.2000000000003\n",
      "1.386138613861386\n",
      "Upper limit:  inf\n",
      "0.47\n"
     ]
    }
   ],
   "source": [
    "likes_count = np.array(X_sample.sum(axis=1)).flatten()\n",
    "cut_points = list()\n",
    "for i in range(1, 5):\n",
    "    cut_points.append(np.percentile(likes_count, i*20))\n",
    "cut_points.append(np.inf)\n",
    "\n",
    "min_likes = 0\n",
    "sample_matches = np.array(sample_matches) \n",
    "for cp in cut_points:\n",
    "    print(\"Upper limit: \", cp)\n",
    "    selected_obs = (likes_count >= min_likes) & (likes_count < cp)\n",
    "    print(sample_matches[selected_obs].mean())\n",
    "    min_likes = cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counterfactual explanations\n",
    "explainer.Explainer(scoring_function, default, prune=False)\n",
    "\n",
    "t0 = time.time()\n",
    "sample_e = explain.explain(X_sample, thresholds=float(threshold), max_ite=50, stop_at_first=True)\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "sample_lens = np.array([len(e[0]) for e in sample_e])\n",
    "min_likes = 0\n",
    "for cp in cut_points:\n",
    "    print(\"Upper limit: \", cp)\n",
    "    selected_obs = (likes_count >= min_likes) & (likes_count < cp)\n",
    "    print(sample_lens[selected_obs].mean())\n",
    "    min_likes = cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out real quantiles\n",
    "likes_count = X_targeted.sum(axis=1)\n",
    "\n",
    "print(np.percentile(likes_count, 20))\n",
    "print(np.percentile(likes_count, 40))\n",
    "print(np.percentile(likes_count, 60))\n",
    "print(np.percentile(likes_count, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Define Cost Function\n",
    "likes_per_page = np.array(sparse_mat.sum(axis=0)).reshape(-1)\n",
    "costs = norm.cdf((np.mean(likes_per_page) - likes_per_page) / np.std(likes_per_page))\n",
    "costs += np.full(X_train.shape[1], 0.1)\n",
    "\n",
    "def likes_cost_func(original_obs, new_obs):\n",
    "    return (original_obs - new_obs).dot(costs)\n",
    "\n",
    "# Method to print explanation details\n",
    "def print_explanation(e):\n",
    "    for p in e:\n",
    "        print(df_names.loc[pages_c[p]].values[0], \"| Likes :\", int(likes_per_page[p]))\n",
    "\n",
    "#  Find explanations with/without costs\n",
    "# Good candidates: 243, 457, 11, 148, 194\n",
    "ixs = [194, 148, 457]\n",
    "t = float(threshold)\n",
    "for i in ixs:\n",
    "    # With cost\n",
    "    cost_e = explain.explain(X_sample[i, :], thresholds=t, stop_at_first=True, cost_func=likes_cost_func)[0][0]\n",
    "    # No cost\n",
    "    no_cost_e = explain.explain(X_sample[i, :], thresholds=t, stop_at_first=True)[0][0]\n",
    "    print(\"ID: \", i)\n",
    "    print(\"Explanation WITHOUT costs\")\n",
    "    print_explanation(no_cost_e)\n",
    "    print(\"---\")\n",
    "    print(\"Explanation WITH costs\")\n",
    "    print_explanation(cost_e)\n",
    "    print(\"*************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Case 1: Explaining decisions rather than predictions\n",
    "\n",
    "\n",
    "Empirical Case 2: High-dimensional and context specific explanations\n",
    "\n",
    "Probabilistic approaches are unreliable when we are dealing with high dimensional spaces. Someone old got targeted with teen stuff, wants to know why. One could show the person the top most informative sites that led to the decision. Turns out those sites are very sensible to the probabilistic procedure. Not a single site appears every time. \n",
    "\n",
    "Systematic analysis. Make four groups of people (only people with at least 15 likes), each one for a quantile. For each group compute SHAP values 5 times. For each observation, count the number of pages that appear in all 5 SHAP values. Get percentage for each (0, 1, 2, 3, 4, and 5 matches). Also record the time it took to compute explanations. \n",
    "\n",
    "As the number of likes increases, it becomes increasingly unlikely to get consistent results, even if the goal is to find only the top most informative attributes rather than estimating precise weights. \n",
    "\n",
    "Our case, an explanation can be found very fast. Do the same analysis showing the average length of explanation for each group.\n",
    "\n",
    "Problem is that, there are so many features that some of them may not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(EXAMPLE 1: Facebook Likes? Predict if someone is young.)\n",
    "- Analysis for fast (vs precise) explanations (case 2). Look for young (or old) person with many likes. Compute SHAP several times to get a distribution of the values. Get average computation time. Compare to computation of EDC (faster, consistent)\n",
    "- Analysis for \"cheapest\" explanations (case 3). Show explanations (many likes) when not sorting by popularity. Compare when sorting by popularity.\n",
    "\n",
    "(EXAMPLE 2: Churn data)\n",
    "- Incorporate decisions from multiple models. Predict monthly charge and whether the person will churn. Idea: Look for guy where top regression feature and top probability feature are different from top combined feature. Again, show how several features influence expected loss, but only a few are necessary to change decision."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
